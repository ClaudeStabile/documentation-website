<!doctype html>
<html lang="fr" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-developpers/apis/ASR/models">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.1.0">
<title data-rh="true">Automatic Speech Recognition Models | LinTO documentation</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doc.linto.ai/fr/docs/developpers/apis/ASR/models"><meta data-rh="true" name="docusaurus_locale" content="fr"><meta data-rh="true" name="docsearch:language" content="fr"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Automatic Speech Recognition Models | LinTO documentation"><meta data-rh="true" name="description" content="By LINAGORA - French, English, Arabic"><meta data-rh="true" property="og:description" content="By LINAGORA - French, English, Arabic"><link data-rh="true" rel="icon" href="/fr/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doc.linto.ai/fr/docs/developpers/apis/ASR/models"><link data-rh="true" rel="alternate" href="https://doc.linto.ai/docs/developpers/apis/ASR/models" hreflang="en"><link data-rh="true" rel="alternate" href="https://doc.linto.ai/fr/docs/developpers/apis/ASR/models" hreflang="fr"><link data-rh="true" rel="alternate" href="https://doc.linto.ai/docs/developpers/apis/ASR/models" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/fr/tutorials/rss.xml" title="LinTO documentation RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/fr/tutorials/atom.xml" title="LinTO documentation Atom Feed"><link rel="stylesheet" href="/fr/assets/css/styles.37847769.css">
<link rel="preload" href="/fr/assets/js/runtime~main.5538cf3c.js" as="script">
<link rel="preload" href="/fr/assets/js/main.81cca2a0.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Aller au contenu principal"><a href="#" class="skipToContent_fXgn">Aller au contenu principal</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/fr/"><div class="navbar__logo"><img src="/fr/img/linto.svg" alt="LinTO" class="themedImage_ToTc themedImage--light_HNdA"><img src="/fr/img/linto.svg" alt="LinTO" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Handbooks </b></a><a class="navbar__item navbar__link" href="/fr/docs/consumers">For users</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/fr/docs/developpers">For developpers</a><a class="navbar__item navbar__link" href="/fr/tutorials">Tutorials</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/linto-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar__item navbar__link header-github-link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Basculer entre le mode sombre et clair (actuellement mode clair)" aria-label="Basculer entre le mode sombre et clair (actuellement mode clair)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Retour au début de la page" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fr/docs/developpers">About</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/fr/docs/developpers/apis">On premises Cognitive APIs</a><button aria-label="Plier/Déplier la catégorie &#x27;On premises Cognitive APIs&#x27; de la barre latérale" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/fr/docs/developpers/apis/ASR">LinTO Platform STT</a><button aria-label="Plier/Déplier la catégorie &#x27;LinTO Platform STT&#x27; de la barre latérale" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/fr/docs/developpers/apis/ASR/dockerrun">Run with Docker</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/fr/docs/developpers/apis/ASR/models">Models download</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/fr/docs/developpers/apis/ASR/platformrun">Run with LinTO platform server</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/fr/docs/developpers/apis/NLP">Natural Language Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/fr/docs/developpers/apis/TTS">Text To Speech</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/fr/docs/developpers/agent">Virtual agents and smart assistants</a><button aria-label="Plier/Déplier la catégorie &#x27;Virtual agents and smart assistants&#x27; de la barre latérale" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fr/docs/developpers/meeting">LinTO for meetings</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/fr/docs/external">Ressources</a><button aria-label="Plier/Déplier la catégorie &#x27;Ressources&#x27; de la barre latérale" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Fil d&#x27;Ariane"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Page d&#x27;accueil" class="breadcrumbs__link" href="/fr/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/fr/docs/developpers/apis"><span itemprop="name">On premises Cognitive APIs</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/fr/docs/developpers/apis/ASR"><span itemprop="name">LinTO Platform STT</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Models download</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">Sur cette page</button></div><div class="theme-doc-markdown markdown"><h1>Automatic Speech Recognition Models</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="by-linagora---french-english-arabic">By LINAGORA - French, English, Arabic<a class="hash-link" href="#by-linagora---french-english-arabic" title="Lien direct vers le titre">​</a></h2><p>We propose models for a few language, but we do it right, achieving beyond state of the art performance and accuracy for French, Arabic and English</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>astuce</div><div class="admonitionContent_S0QG"><p>Those models are the most generic ones, achieving best all-over performance, we however maintain specific accoustic models for business use-cases like heavily noisy environment, aeroplanes, phones, call-centers and decoding graphs for specific vocabulary, like medical or banking... contact us to learn more.</p></div></div><div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">French v2</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">French v1</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">English US</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Arabic</li></ul><div class="margin-top--md"><div value="French v2" label="French v2"><h4 class="anchor anchorWithStickyNavbar_LWe7" id="acoustic-model">Acoustic model<a class="hash-link" href="#acoustic-model" title="Lien direct vers le titre">​</a></h4><ul><li>A deep Time Delay Neural Network (TDNN) model, trained on a large spontanious speech corpora. Data augmentation was applied to increase the quantity of training data and to simulate artificially some environment conditions (noise, speaker). The full corpus after data augmentation is approximately 7100 hours.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/acoustic-models/fr-FR/linSTT_AM_fr-FR_v2.0.0.zip" target="_blank" rel="noopener noreferrer">2.0.0 AM download</a></p><ul><li>A deep neural network architecture (~30M parameters). This model is trained on the same data (7100 hours).</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/acoustic-models/fr-FR/linSTT_AM_fr-FR_v2.2.0.zip" target="_blank" rel="noopener noreferrer">2.2.0 AM download</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="decoding-graph">Decoding graph<a class="hash-link" href="#decoding-graph" title="Lien direct vers le titre">​</a></h4><ul><li>This model is trained on multiple text corpus from different resources. It requires important memory resource on the one hand and provides very accurate transcription.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Medium_v2.1.0.zip" target="_blank" rel="noopener noreferrer">2.1.0 LM download</a></p><ul><li>This model is trained on various large corpus. Should provide best accuracy but is a bit more resource intensive than the other models.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Big_v2.2.0.zip" target="_blank" rel="noopener noreferrer">2.2.0 LM download</a></p></div><div value="French v1" label="French v1" hidden=""><h4 class="anchor anchorWithStickyNavbar_LWe7" id="acoustic-model-1">Acoustic model<a class="hash-link" href="#acoustic-model-1" title="Lien direct vers le titre">​</a></h4><ul><li>A deep Time Delay Neural Network (TDNN) model, trained on a 1700 hours of spontanious speech corpora. It has a background noise resistance. A speaker adaptation model is used to have robust predictions among speaker variability.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/acoustic-models/fr-FR/linSTT_AM_fr-FR_v1.0.0.zip" target="_blank" rel="noopener noreferrer">1.0.0 AM download</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="decoding-graph-1">Decoding graph<a class="hash-link" href="#decoding-graph-1" title="Lien direct vers le titre">​</a></h4><ul><li>This model is trained on small corpus. It is a small Model (100Mo) which generates an acceptable transcription but is very suitable for use in embedded applications.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Small_v1.1.0.zip" target="_blank" rel="noopener noreferrer">1.1.0 LM download</a></p><ul><li>This model is trained on much more corpus than the small one. It requires important memory resource on the one hand and provides very accurate transcription.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Medium_v1.2.0.zip" target="_blank" rel="noopener noreferrer">1.2.0 LM download</a></p><ul><li>This model is trained on various large corpus. Should provide the best accuracy but is a bit more resource intensive than the other models.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Big_v1.3.0.zip" target="_blank" rel="noopener noreferrer">1.3.0 LM download</a></p></div><div value="English US" label="English US" hidden=""><h4 class="anchor anchorWithStickyNavbar_LWe7" id="acoustic-model-2">Acoustic model<a class="hash-link" href="#acoustic-model-2" title="Lien direct vers le titre">​</a></h4><ul><li>A chain model based on TDNN-F, trained on a 1000 hours with volume and speed perturbation.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/acoustic-models/en-US/linSTT_AM_en-US_v1.0.0.zip" target="_blank" rel="noopener noreferrer">v1 AM download</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="decoding-graph-2">Decoding graph<a class="hash-link" href="#decoding-graph-2" title="Lien direct vers le titre">​</a></h4><ul><li>Two language model are used for the decoding. A <strong>medium model</strong> is used to perform the decoding pass. A <strong>big model</strong>, trained on a large corpus of books, is used to perform the rescoring pass.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/en-US/decoding_graph_en-US_v1.1.0.zip" target="_blank" rel="noopener noreferrer">v1.1 LM download</a></p><ul><li>This model is trained on various large corpus. Should provide the best accuracy but is a bit more resource intensive than the other models.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/en-US/decoding_graph_en-US_Big_v1.2.0.zip" target="_blank" rel="noopener noreferrer">v1.2 LM download</a></p></div><div value="Arabic" label="Arabic" hidden=""><h4 class="anchor anchorWithStickyNavbar_LWe7" id="acoustic-model-3">Acoustic model<a class="hash-link" href="#acoustic-model-3" title="Lien direct vers le titre">​</a></h4><ul><li>A chain model based on  deep Time Delay Neural Network (TDNN), trained on a 1200 hours of Arabic broadcast data.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/acoustic-models/ar-AR/LinSTT_AM_ar-AR_v1.0.0.zip" target="_blank" rel="noopener noreferrer">v1 AM download</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="decoding-graph-3">Decoding graph<a class="hash-link" href="#decoding-graph-3" title="Lien direct vers le titre">​</a></h4><ul><li>This language model is trained on small arabic text corpus. It is trained with SRILM tools.</li></ul><p><a href="https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/ar-AR/decoding_graph_ar-AR_v1.1.0.zip" target="_blank" rel="noopener noreferrer">v1.1 LM download</a></p></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-built-models--other-languages">Community built models &amp; Other languages<a class="hash-link" href="#community-built-models--other-languages" title="Lien direct vers le titre">​</a></h2><table class="table table-bordered"><thead><tr><th>Model</th><th>Size</th><th>Word error rate/Speed</th><th>Notes</th><th>License</th></tr></thead><tbody><tr><td><strong>English</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-en-us-0.15</a></td><td>40M</td><td>9.85 (librispeech test-clean) 10.38 (tedlium)</td><td>Lightweight wideband model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-us-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-us-0.22</a></td><td>1.8G</td><td>5.69 (librispeech test-clean) 6.05 (tedlium) 29.78(callcenter)</td><td>Accurate generic US English model</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-us-0.22-lgraph.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-us-0.22-lgraph</a></td><td>128M</td><td>7.82 (librispeech) 8.20 (tedlium)</td><td>Big US English model with dynamic graph</td><td>Apache 2.0</td></tr><tr><td><strong>English Other</strong></td><td> </td><td><strong>Older Models</strong></td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-us-daanzu-20200905.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-us-daanzu-20200905</a></td><td>1.0G</td><td>7.08 (librispeech test-clean)  8.25 (tedlium)</td><td>Wideband model for dictation from <a href="https://github.com/daanzu/kaldi-active-grammar" target="_blank" rel="noopener noreferrer">Kaldi-active-grammar</a> project</td><td>AGPL</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-us-daanzu-20200905-lgraph.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-us-daanzu-20200905-lgraph</a></td><td>129M</td><td>8.20 (librispeech test-clean) 9.28 (tedlium)</td><td>Wideband model for dictation from <a href="https://github.com/daanzu/kaldi-active-grammar" target="_blank" rel="noopener noreferrer">Kaldi-active-grammar</a> project with configurable graph</td><td>AGPL</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-us-librispeech-0.2.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-us-librispeech-0.2</a></td><td>845M</td><td>TBD</td><td>Repackaged Librispeech model from <a href="https://kaldi-asr.org/models/m13" target="_blank" rel="noopener noreferrer">Kaldi</a>, not very accurate</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-en-us-zamia-0.5.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-en-us-zamia-0.5</a></td><td>49M</td><td>11.55 (librispeech test-clean) 12.64 (tedlium)</td><td>Repackaged Zamia model f_250, mainly for research</td><td>LGPL-3.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-us-aspire-0.2.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-us-aspire-0.2</a></td><td>1.4G</td><td>13.64 (librispeech test-clean) 12.89 (tedlium) 33.82(callcenter)</td><td>Kaldi original ASPIRE model, not very accurate</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-us-0.21.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-us-0.21</a></td><td>1.6G</td><td>5.43 (librispeech test-clean) 6.42 (tedlium) 40.63(callcenter)</td><td>Wideband model previous generation</td><td>Apache 2.0</td></tr><tr><td><strong>Indian English</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-en-in-0.5.zip" target="_blank" rel="noopener noreferrer">vosk-model-en-in-0.5</a></td><td>1G</td><td>36.12 (NPTEL Pure)</td><td>Generic Indian English model for telecom and broadcast</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-en-in-0.4.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-en-in-0.4</a></td><td>36M</td><td>49.05 (NPTEL Pure)</td><td>Lightweight Indian English model for mobile applications</td><td>Apache 2.0</td></tr><tr><td><strong>Chinese</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-cn-0.22</a></td><td>42M</td><td>23.54 (SpeechIO-02) 38.29 (SpeechIO-06) 17.15 (THCHS)</td><td>Lightweight model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-cn-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-cn-0.22</a></td><td>1.3G</td><td>13.98 (SpeechIO-02) 27.30 (SpeechIO-06) 7.43 (THCHS)</td><td>Big generic Chinese model for server processing</td><td>Apache 2.0</td></tr><tr><td><strong>Chinese Other</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-cn-kaldi-multicn-0.15.zip" target="_blank" rel="noopener noreferrer">vosk-model-cn-kaldi-multicn-0.15</a></td><td>1.5G</td><td>17.44 (SpeechIO-02) 9.56 (THCHS)</td><td>Original Wideband Kaldi multi-cn model from <a href="https://kaldi-asr.org/models/m11" target="_blank" rel="noopener noreferrer">Kaldi</a> with Vosk LM</td><td>Apache 2.0</td></tr><tr><td><strong>Russian</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-ru-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-ru-0.22</a></td><td>1.5G</td><td>5.74 (our audiobooks) 13.35 (open_stt audiobooks) 20.73 (open_stt youtube) 37.38 (openstt calls) 8.65 (golos crowd) 19.71 (sova devices)</td><td>Big mixed band Russian model for server processing</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-ru-0.22</a></td><td>45M</td><td>22.71 (openstt audiobooks) 31.97 (openstt youtube) 29.89 (sova devices) 11.79 (golos crowd)</td><td>Lightweight wideband model for Android/iOS and RPi</td><td>Apache 2.0</td></tr><tr><td><strong>Russian Other</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-ru-0.10.zip" target="_blank" rel="noopener noreferrer">vosk-model-ru-0.10</a></td><td>2.5G</td><td>5.71 (our audiobooks) 16.26 (open_stt audiobooks) 26.20 (public_youtube_700_val open_stt) 40.15 (asr_calls_2_val open_stt)</td><td>Big narrowband Russian model for server processing</td><td>Apache 2.0</td></tr><tr><td><strong>French</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-fr-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-fr-0.22</a></td><td>41M</td><td>23.95 (cv test) 19.30 (mtedx) 27.25 (podcast)</td><td>Lightweight wideband model for Android/iOS and RPi</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-fr-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-fr-0.22</a></td><td>1.4G</td><td>14.72 (cv test) 11.64 (mls) 13.10 (mtedx) 21.61 (podcast) 13.22 (voxpopuli)</td><td>Big accurate model for servers</td><td>Apache 2.0</td></tr><tr><td><strong>French Other</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-fr-pguyot-0.3.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-fr-pguyot-0.3</a></td><td>39M</td><td>37.04 (cv test) 28.72 (mtedx) 37.46 (podcast)</td><td>Lightweight wideband model for Android and RPi trained by <a href="https://github.com/pguyot/zamia-speech/releases" target="_blank" rel="noopener noreferrer">Paul Guyot</a></td><td>CC-BY-NC-SA 4.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-fr-0.6-linto-2.2.0.zip" target="_blank" rel="noopener noreferrer">vosk-model-fr-0.6-linto-2.2.0</a></td><td>1.5G</td><td>16.19 (cv test) 16.44 (mtedx) 23.77 (podcast) 0.4xRT</td><td>Model from <a href="https://doc.linto.ai/#/services/linstt" target="_blank" rel="noopener noreferrer">LINTO</a> project</td><td>AGPL</td></tr><tr><td><strong>German</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-de-0.21.zip" target="_blank" rel="noopener noreferrer">vosk-model-de-0.21</a></td><td>1.9G</td><td>9.83 (Tuda-de test), 24.00 (podcast) 12.82 (cv-test) 12.42 (mls) 33.26 (mtedx)</td><td>Big German model for telephony and server</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-de-tuda-0.6-900k.zip" target="_blank" rel="noopener noreferrer">vosk-model-de-tuda-0.6-900k</a></td><td>4.4G</td><td>9.48 (Tuda-de test), 25.82 (podcast) 4.97 (cv-test) 11.01 (mls) 35.20 (mtedx)</td><td>Latest big wideband model from <a href="https://github.com/uhh-lt/kaldi-tuda-de" target="_blank" rel="noopener noreferrer">Tuda-DE</a> project</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-de-zamia-0.3.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-de-zamia-0.3</a></td><td>49M</td><td>14.81 (Tuda-de test, 37.46 (podcast)</td><td>Zamia f_250 small model repackaged (not recommended)</td><td>LGPL-3.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-de-0.15.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-de-0.15</a></td><td>45M</td><td>13.75 (Tuda-de test), 30.67 (podcast)</td><td>Lightweight wideband model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><strong>Spanish</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-es-0.42.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-es-0.42</a></td><td>39M</td><td>16.02 (cv test) 16.72 (mtedx test) 11.21 (mls)</td><td>Lightweight wideband model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-es-0.42.zip" target="_blank" rel="noopener noreferrer">vosk-model-es-0.42</a></td><td>1.4G</td><td>7.50 (cv test) 10.05 (mtedx test) 5.84 (mls)</td><td>Big model for Spanish</td><td>Apache 2.0</td></tr><tr><td><strong>Portuguese/Brazilian Portuguese</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-pt-0.3.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-pt-0.3</a></td><td>31M</td><td>68.92 (coraa dev) 32.60 (cv test)</td><td>Lightweight wideband model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-pt-fb-v0.1.1-20220516_2113.zip" target="_blank" rel="noopener noreferrer">vosk-model-pt-fb-v0.1.1-20220516_2113</a></td><td>1.6G</td><td>54.34 (coraa dev) 27.70 (cv test)</td><td>Big model from <a href="https://gitlab.com/fb-resources/kaldi-br" target="_blank" rel="noopener noreferrer">FalaBrazil</a></td><td>GPLv3.0</td></tr><tr><td><strong>Greek</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-el-gr-0.7.zip" target="_blank" rel="noopener noreferrer">vosk-model-el-gr-0.7</a></td><td>1.1G</td><td>TBD</td><td>Big narrowband Greek model for server processing, not extremely accurate though</td><td>Apache 2.0</td></tr><tr><td><strong>Turkish</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-tr-0.3.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-tr-0.3</a></td><td>35M</td><td>TBD</td><td>Lightweight wideband model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><strong>Vietnamese</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-vn-0.3.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-vn-0.3</a></td><td>32M</td><td>TBD</td><td>Lightweight wideband model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><strong>Italian</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-it-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-it-0.22</a></td><td>48M</td><td>16.88 (cv test) 25.87 (mls) 17.01 (mtedx)</td><td>Lightweight model for Android and RPi</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-it-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-it-0.22</a></td><td>1.2G</td><td>8.10 (cv test) 15.68 (mls) 11.23 (mtedx)</td><td>Big generic Italian model for servers</td><td>Apache 2.0</td></tr><tr><td><strong>Dutch</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-nl-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-nl-0.22</a></td><td>39M</td><td>22.45 (cv test) 26.80 (tv) 25.84 (mls) 24.09 (voxpopuli)</td><td>Lightweight model for Dutch</td><td>Apache 2.0</td></tr><tr><td><strong>Dutch Other</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-nl-spraakherkenning-0.6.zip" target="_blank" rel="noopener noreferrer">vosk-model-nl-spraakherkenning-0.6</a></td><td>860M</td><td>20.40 (cv test) 32.64 (tv) 17.73 (mls) 19.96 (voxpopuli)</td><td>Medium Dutch model from <a href="https://github.com/opensource-spraakherkenning-nl/Kaldi_NL" target="_blank" rel="noopener noreferrer">Kaldi_NL</a></td><td>CC-BY-NC-SA</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-nl-spraakherkenning-0.6-lgraph.zip" target="_blank" rel="noopener noreferrer">vosk-model-nl-spraakherkenning-0.6-lgraph</a></td><td>100M</td><td>22.82 (cv test) 34.01 (tv) 18.81 (mls) 21.01 (voxpopuli)</td><td>Smaller model with dynamic graph</td><td>CC-BY-NC-SA</td></tr><tr><td><strong>Catalan</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-ca-0.4.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-ca-0.4</a></td><td>42M</td><td>TBD</td><td>Lightweight wideband model for Android and RPi for Catalan</td><td>Apache 2.0</td></tr><tr><td><strong>Arabic</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-ar-mgb2-0.4.zip" target="_blank" rel="noopener noreferrer">vosk-model-ar-mgb2-0.4</a></td><td>318M</td><td>16.40 (MGB-2 dev set)</td><td>Repackaged Arabic model trained on MGB2 dataset from <a href="https://kaldi-asr.org/models/m9" target="_blank" rel="noopener noreferrer">Kaldi</a></td><td>Apache 2.0</td></tr><tr><td><strong>Farsi</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-fa-0.4.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-fa-0.4</a></td><td>47M</td><td>TBD</td><td>Lightweight wideband model for Android and RPi for Farsi (Persian)</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-fa-0.5.zip" target="_blank" rel="noopener noreferrer">vosk-model-fa-0.5</a></td><td>1G</td><td>TBD</td><td>Model with large vocabulary, not yet accurate but better than before (Persian)</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-fa-0.5.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-fa-0.5</a></td><td>60M</td><td>TBD</td><td>Bigger small model for desktop application (Persian)</td><td>Apache 2.0</td></tr><tr><td><strong>Filipino</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-tl-ph-generic-0.6.zip" target="_blank" rel="noopener noreferrer">vosk-model-tl-ph-generic-0.6</a></td><td>320M</td><td>TBD</td><td>Medium wideband model for Filipino (Tagalog) by <a href="https://github.com/feddybear/flipside_ph" target="_blank" rel="noopener noreferrer">feddybear</a></td><td>CC-BY-NC-SA 4.0</td></tr><tr><td><strong>Ukrainian</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-uk-v3-nano.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-uk-v3-nano</a></td><td>73M</td><td>TBD</td><td>Nano model from <a href="https://github.com/egorsmkv/speech-recognition-uk" target="_blank" rel="noopener noreferrer">Speech Recognition for Ukrainian</a></td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-uk-v3-small.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-uk-v3-small</a></td><td>133M</td><td>TBD</td><td>Small model from <a href="https://github.com/egorsmkv/speech-recognition-uk" target="_blank" rel="noopener noreferrer">Speech Recognition for Ukrainian</a></td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-uk-v3.zip" target="_blank" rel="noopener noreferrer">vosk-model-uk-v3</a></td><td>343M</td><td>TBD</td><td>Bigger model from <a href="https://github.com/egorsmkv/speech-recognition-uk" target="_blank" rel="noopener noreferrer">Speech Recognition for Ukrainian</a></td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-uk-v3-lgraph.zip" target="_blank" rel="noopener noreferrer">vosk-model-uk-v3-lgraph</a></td><td>325M</td><td>TBD</td><td>Big dynamic model from <a href="https://github.com/egorsmkv/speech-recognition-uk" target="_blank" rel="noopener noreferrer">Speech Recognition for Ukrainian</a></td><td>Apache 2.0</td></tr><tr><td><strong>Kazakh</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-kz-0.15.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-kz-0.15</a></td><td>42M</td><td>9.60(dev) 8.32(test)</td><td>Small mobile model from <a href="https://github.com/IS2AI/ISSAI_SAIDA_Kazakh_ASR" target="_blank" rel="noopener noreferrer">SAIDA_Kazakh</a></td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-kz-0.15.zip" target="_blank" rel="noopener noreferrer">vosk-model-kz-0.15</a></td><td>378M</td><td>8.06(dev) 6.81(test)</td><td>Bigger wideband model <a href="https://github.com/IS2AI/ISSAI_SAIDA_Kazakh_ASR" target="_blank" rel="noopener noreferrer">SAIDA_Kazakh</a></td><td>Apache 2.0</td></tr><tr><td><strong>Swedish</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-sv-rhasspy-0.15.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-sv-rhasspy-0.15</a></td><td>289M</td><td>TBD</td><td>Repackaged model from <a href="https://github.com/rhasspy/sv_kaldi-rhasspy" target="_blank" rel="noopener noreferrer">Rhasspy project</a></td><td>MIT</td></tr><tr><td><strong>Japanese</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-ja-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-ja-0.22</a></td><td>48M</td><td>9.52(csj CER) 17.07(ted10k CER)</td><td>Lightweight wideband model for Japanese</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-ja-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-ja-0.22</a></td><td>1Gb</td><td>8.40(csj CER) 13.91(ted10k CER)</td><td>Big model for Japanese</td><td>Apache 2.0</td></tr><tr><td><strong>Esperanto</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-eo-0.42.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-eo-0.42</a></td><td>42M</td><td>7.24 (CV Test)</td><td>Lightweight model for Esperanto</td><td>Apache 2.0</td></tr><tr><td><strong>Hindi</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-hi-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-hi-0.22</a></td><td>42M</td><td>20.89 (IITM Challenge) 24.72 (MUCS Challenge)</td><td>Lightweight model for Hindi</td><td>Apache 2.0</td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-hi-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-hi-0.22</a></td><td>1.5Gb</td><td>14.85 (CV Test) 14.83 (IITM Challenge) 13.11 (MUCS Challenge)</td><td>Big accurate model for servers</td><td>Apache 2.0</td></tr><tr><td><strong>Czech</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-cs-0.4-rhasspy.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-cs-0.4-rhasspy</a></td><td>44M</td><td>21.29 (CV Test)</td><td>Lightweight model for Czech from Rhasspy project</td><td>MIT</td></tr><tr><td><strong>Polish</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-small-pl-0.22.zip" target="_blank" rel="noopener noreferrer">vosk-model-small-pl-0.22</a></td><td>50.5M</td><td>18.36 (CV Test) 16.88 (MLS Test) 11.55 (Voxpopuli Test)</td><td>Lightweight model for Polish for Android</td><td>Apache 2.0</td></tr><tr><td><strong>Speaker identification model</strong></td><td> </td><td> </td><td> </td><td> </td></tr><tr><td><a href="https://alphacephei.com/vosk/models/vosk-model-spk-0.4.zip" target="_blank" rel="noopener noreferrer">vosk-model-spk-0.4</a></td><td>13M</td><td>TBD</td><td>Model for speaker identification, should work for all languages</td><td>Apache 2.0</td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/linto-ai/documentation-website/tree/source/docs/developpers/apis/ASR/models.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Éditer cette page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Pagination des documents"><a class="pagination-nav__link pagination-nav__link--prev" href="/fr/docs/developpers/apis/ASR/dockerrun"><div class="pagination-nav__sublabel">Précédent</div><div class="pagination-nav__label">Run with Docker</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/fr/docs/developpers/apis/ASR/platformrun"><div class="pagination-nav__sublabel">Suivant</div><div class="pagination-nav__label">Run with LinTO platform server</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#by-linagora---french-english-arabic" class="table-of-contents__link toc-highlight">By LINAGORA - French, English, Arabic</a></li><li><a href="#community-built-models--other-languages" class="table-of-contents__link toc-highlight">Community built models &amp; Other languages</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Developper&#x27;s docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/fr/docs/developpers/apis">Cognitive APIs</a></li><li class="footer__item"><a class="footer__link-item" href="/fr/docs/developpers/agent">Virtual Agents and Smart Assistants</a></li><li class="footer__item"><a class="footer__link-item" href="/fr/docs/developpers/meeting">Smart Conversations</a></li><li class="footer__item"><a class="footer__link-item" href="/fr/tutorials">Tutorials</a></li></ul></div><div class="col footer__col"><div class="footer__title">LinTO Live</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://linagora.com/fr/produits/linto/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Browser demos</a></li><li class="footer__item"><a href="https://convos.linto.ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Conversation Manager</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/jvNK3FXv3d" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://linto.ai/#contact" target="_blank" rel="noopener noreferrer" class="footer__link-item">Contact</a></li></ul></div><div class="col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/linto-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://linto.ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinTO HomePage<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://linagora.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">LINAGORA<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://labs.linagora.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">LINAGORA Labs - Meet the LinTO team<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 LINAGORA</div></div></div></footer></div>
<script src="/fr/assets/js/runtime~main.5538cf3c.js"></script>
<script src="/fr/assets/js/main.81cca2a0.js"></script>
</body>
</html>