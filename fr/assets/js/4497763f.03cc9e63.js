"use strict";(self.webpackChunknewdoc=self.webpackChunknewdoc||[]).push([[5422],{3905:(t,l,e)=>{e.d(l,{Zo:()=>i,kt:()=>m});var n=e(7294);function o(t,l,e){return l in t?Object.defineProperty(t,l,{value:e,enumerable:!0,configurable:!0,writable:!0}):t[l]=e,t}function d(t,l){var e=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);l&&(n=n.filter((function(l){return Object.getOwnPropertyDescriptor(t,l).enumerable}))),e.push.apply(e,n)}return e}function a(t){for(var l=1;l<arguments.length;l++){var e=null!=arguments[l]?arguments[l]:{};l%2?d(Object(e),!0).forEach((function(l){o(t,l,e[l])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(e)):d(Object(e)).forEach((function(l){Object.defineProperty(t,l,Object.getOwnPropertyDescriptor(e,l))}))}return t}function r(t,l){if(null==t)return{};var e,n,o=function(t,l){if(null==t)return{};var e,n,o={},d=Object.keys(t);for(n=0;n<d.length;n++)e=d[n],l.indexOf(e)>=0||(o[e]=t[e]);return o}(t,l);if(Object.getOwnPropertySymbols){var d=Object.getOwnPropertySymbols(t);for(n=0;n<d.length;n++)e=d[n],l.indexOf(e)>=0||Object.prototype.propertyIsEnumerable.call(t,e)&&(o[e]=t[e])}return o}var s=n.createContext({}),u=function(t){var l=n.useContext(s),e=l;return t&&(e="function"==typeof t?t(l):a(a({},l),t)),e},i=function(t){var l=u(t.components);return n.createElement(s.Provider,{value:l},t.children)},k={inlineCode:"code",wrapper:function(t){var l=t.children;return n.createElement(n.Fragment,{},l)}},p=n.forwardRef((function(t,l){var e=t.components,o=t.mdxType,d=t.originalType,s=t.parentName,i=r(t,["components","mdxType","originalType","parentName"]),p=u(e),m=o,h=p["".concat(s,".").concat(m)]||p[m]||k[m]||d;return e?n.createElement(h,a(a({ref:l},i),{},{components:e})):n.createElement(h,a({ref:l},i))}));function m(t,l){var e=arguments,o=l&&l.mdxType;if("string"==typeof t||o){var d=e.length,a=new Array(d);a[0]=p;var r={};for(var s in l)hasOwnProperty.call(l,s)&&(r[s]=l[s]);r.originalType=t,r.mdxType="string"==typeof t?t:o,a[1]=r;for(var u=2;u<d;u++)a[u]=e[u];return n.createElement.apply(null,a)}return n.createElement.apply(null,e)}p.displayName="MDXCreateElement"},5162:(t,l,e)=>{e.d(l,{Z:()=>a});var n=e(7294),o=e(6010);const d="tabItem_Ymn6";function a(t){let{children:l,hidden:e,className:a}=t;return n.createElement("div",{role:"tabpanel",className:(0,o.Z)(d,a),hidden:e},l)}},5488:(t,l,e)=>{e.d(l,{Z:()=>m});var n=e(7462),o=e(7294),d=e(6010),a=e(2389),r=e(7392),s=e(7094),u=e(2466);const i="tabList__CuJ",k="tabItem_LNqP";function p(t){var l;const{lazy:e,block:a,defaultValue:p,values:m,groupId:h,className:c}=t,v=o.Children.map(t.children,(t=>{if((0,o.isValidElement)(t)&&"value"in t.props)return t;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof t.type?t.type:t.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),g=m??v.map((t=>{let{props:{value:l,label:e,attributes:n}}=t;return{value:l,label:e,attributes:n}})),f=(0,r.l)(g,((t,l)=>t.value===l.value));if(f.length>0)throw new Error(`Docusaurus error: Duplicate values "${f.map((t=>t.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const b=null===p?p:p??(null==(l=v.find((t=>t.props.default)))?void 0:l.props.value)??v[0].props.value;if(null!==b&&!g.some((t=>t.value===b)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${b}" but none of its children has the corresponding value. Available values are: ${g.map((t=>t.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:A,setTabGroupChoices:w}=(0,s.U)(),[y,T]=(0,o.useState)(b),z=[],{blockElementScrollPositionUntilNextRender:S}=(0,u.o5)();if(null!=h){const t=A[h];null!=t&&t!==y&&g.some((l=>l.value===t))&&T(t)}const M=t=>{const l=t.currentTarget,e=z.indexOf(l),n=g[e].value;n!==y&&(S(l),T(n),null!=h&&w(h,String(n)))},R=t=>{var l;let e=null;switch(t.key){case"ArrowRight":{const l=z.indexOf(t.currentTarget)+1;e=z[l]??z[0];break}case"ArrowLeft":{const l=z.indexOf(t.currentTarget)-1;e=z[l]??z[z.length-1];break}}null==(l=e)||l.focus()};return o.createElement("div",{className:(0,d.Z)("tabs-container",i)},o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,d.Z)("tabs",{"tabs--block":a},c)},g.map((t=>{let{value:l,label:e,attributes:a}=t;return o.createElement("li",(0,n.Z)({role:"tab",tabIndex:y===l?0:-1,"aria-selected":y===l,key:l,ref:t=>z.push(t),onKeyDown:R,onFocus:M,onClick:M},a,{className:(0,d.Z)("tabs__item",k,null==a?void 0:a.className,{"tabs__item--active":y===l})}),e??l)}))),e?(0,o.cloneElement)(v.filter((t=>t.props.value===y))[0],{className:"margin-top--md"}):o.createElement("div",{className:"margin-top--md"},v.map(((t,l)=>(0,o.cloneElement)(t,{key:l,hidden:t.props.value!==y})))))}function m(t){const l=(0,a.Z)();return o.createElement(p,(0,n.Z)({key:String(l)},t))}},7191:(t,l,e)=>{e.r(l),e.d(l,{assets:()=>u,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>i});var n=e(7462),o=(e(7294),e(3905)),d=e(5488);e(5162);const a={sidebar_label:"Models download",sidebar_position:2},r="Automatic Speech Recognition Models",s={unversionedId:"developpers/apis/ASR/models",id:"developpers/apis/ASR/models",title:"Automatic Speech Recognition Models",description:"By LINAGORA - French, English, Arabic",source:"@site/docs/developpers/apis/ASR/models.md",sourceDirName:"developpers/apis/ASR",slug:"/developpers/apis/ASR/models",permalink:"/fr/docs/developpers/apis/ASR/models",draft:!1,editUrl:"https://github.com/linto-ai/documentation-website/tree/source/docs/developpers/apis/ASR/models.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_label:"Models download",sidebar_position:2},sidebar:"devSidebar",previous:{title:"Run with Docker",permalink:"/fr/docs/developpers/apis/ASR/dockerrun"},next:{title:"Run with LinTO platform server",permalink:"/fr/docs/developpers/apis/ASR/platformrun"}},u={},i=[{value:"By LINAGORA - French, English, Arabic",id:"by-linagora---french-english-arabic",level:2},{value:"Acoustic model",id:"acoustic-model",level:4},{value:"Decoding graph",id:"decoding-graph",level:4},{value:"Acoustic model",id:"acoustic-model-1",level:4},{value:"Decoding graph",id:"decoding-graph-1",level:4},{value:"Acoustic model",id:"acoustic-model-2",level:4},{value:"Decoding graph",id:"decoding-graph-2",level:4},{value:"Acoustic model",id:"acoustic-model-3",level:4},{value:"Decoding graph",id:"decoding-graph-3",level:4},{value:"Community built models &amp; Other languages",id:"community-built-models--other-languages",level:2}],k=(p="Tabitem",function(t){return console.warn("Component "+p+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",t)});var p;const m={toc:i};function h(t){let{components:l,...e}=t;return(0,o.kt)("wrapper",(0,n.Z)({},m,e,{components:l,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"automatic-speech-recognition-models"},"Automatic Speech Recognition Models"),(0,o.kt)("h2",{id:"by-linagora---french-english-arabic"},"By LINAGORA - French, English, Arabic"),(0,o.kt)("p",null,"We propose models for a few language, but we do it right, achieving beyond state of the art performance and accuracy for French, Arabic and English"),(0,o.kt)("admonition",{type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"Those models are the most generic ones, achieving best all-over performance, we however maintain specific accoustic models for business use-cases like heavily noisy environment, aeroplanes, phones, call-centers and decoding graphs for specific vocabulary, like medical or banking... contact us to learn more.")),(0,o.kt)(d.Z,{mdxType:"Tabs"},(0,o.kt)(k,{value:"French v2",label:"French v2",mdxType:"Tabitem"},(0,o.kt)("h4",{id:"acoustic-model"},"Acoustic model"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A deep Time Delay Neural Network (TDNN) model, trained on a large spontanious speech corpora. Data augmentation was applied to increase the quantity of training data and to simulate artificially some environment conditions (noise, speaker). The full corpus after data augmentation is approximately 7100 hours.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/acoustic-models/fr-FR/linSTT_AM_fr-FR_v2.0.0.zip"},"2.0.0 AM download")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A deep neural network architecture (~30M parameters). This model is trained on the same data (7100 hours).")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/acoustic-models/fr-FR/linSTT_AM_fr-FR_v2.2.0.zip"},"2.2.0 AM download")),(0,o.kt)("h4",{id:"decoding-graph"},"Decoding graph"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This model is trained on multiple text corpus from different resources. It requires important memory resource on the one hand and provides very accurate transcription.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Medium_v2.1.0.zip"},"2.1.0 LM download")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This model is trained on various large corpus. Should provide best accuracy but is a bit more resource intensive than the other models.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Big_v2.2.0.zip"},"2.2.0 LM download"))),(0,o.kt)(k,{value:"French v1",label:"French v1",mdxType:"Tabitem"},(0,o.kt)("h4",{id:"acoustic-model-1"},"Acoustic model"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A deep Time Delay Neural Network (TDNN) model, trained on a 1700 hours of spontanious speech corpora. It has a background noise resistance. A speaker adaptation model is used to have robust predictions among speaker variability.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/acoustic-models/fr-FR/linSTT_AM_fr-FR_v1.0.0.zip"},"1.0.0 AM download")),(0,o.kt)("h4",{id:"decoding-graph-1"},"Decoding graph"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This model is trained on small corpus. It is a small Model (100Mo) which generates an acceptable transcription but is very suitable for use in embedded applications.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Small_v1.1.0.zip"},"1.1.0 LM download")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This model is trained on much more corpus than the small one. It requires important memory resource on the one hand and provides very accurate transcription.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Medium_v1.2.0.zip"},"1.2.0 LM download")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This model is trained on various large corpus. Should provide the best accuracy but is a bit more resource intensive than the other models.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/fr-FR/decoding_graph_fr-FR_Big_v1.3.0.zip"},"1.3.0 LM download"))),(0,o.kt)(k,{value:"English US",label:"English US",mdxType:"Tabitem"},(0,o.kt)("h4",{id:"acoustic-model-2"},"Acoustic model"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A chain model based on TDNN-F, trained on a 1000 hours with volume and speed perturbation.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/acoustic-models/en-US/linSTT_AM_en-US_v1.0.0.zip"},"v1 AM download")),(0,o.kt)("h4",{id:"decoding-graph-2"},"Decoding graph"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Two language model are used for the decoding. A ",(0,o.kt)("strong",{parentName:"li"},"medium model")," is used to perform the decoding pass. A ",(0,o.kt)("strong",{parentName:"li"},"big model"),", trained on a large corpus of books, is used to perform the rescoring pass.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/en-US/decoding_graph_en-US_v1.1.0.zip"},"v1.1 LM download")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This model is trained on various large corpus. Should provide the best accuracy but is a bit more resource intensive than the other models.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/en-US/decoding_graph_en-US_Big_v1.2.0.zip"},"v1.2 LM download"))),(0,o.kt)(k,{value:"Arabic",label:"Arabic",mdxType:"Tabitem"},(0,o.kt)("h4",{id:"acoustic-model-3"},"Acoustic model"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A chain model based on  deep Time Delay Neural Network (TDNN), trained on a 1200 hours of Arabic broadcast data.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/acoustic-models/ar-AR/LinSTT_AM_ar-AR_v1.0.0.zip"},"v1 AM download")),(0,o.kt)("h4",{id:"decoding-graph-3"},"Decoding graph"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This language model is trained on small arabic text corpus. It is trained with SRILM tools.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/ar-AR/decoding_graph_ar-AR_v1.1.0.zip"},"v1.1 LM download")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"This language model is trained on big arabic text from different corpus. It is trained with SRILM tools.")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://dl.linto.ai/downloads/model-distribution/decoding-graphs/LVCSR/ar-AR/decoding_graph_ar-AR_v1.2.0.zip"},"v1.2 LM download")))),(0,o.kt)("h2",{id:"community-built-models--other-languages"},"Community built models & Other languages"),(0,o.kt)("table",{class:"table table-bordered"},(0,o.kt)("thead",null,(0,o.kt)("tr",null,(0,o.kt)("th",null,"Model"),(0,o.kt)("th",null,"Size"),(0,o.kt)("th",null,"Word error rate/Speed"),(0,o.kt)("th",null,"Notes"),(0,o.kt)("th",null,"License"))),(0,o.kt)("tbody",null,(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"English")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip"},"vosk-model-small-en-us-0.15")),(0,o.kt)("td",null,"40M"),(0,o.kt)("td",null,"9.85 (librispeech test-clean) 10.38 (tedlium)"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-us-0.22.zip"},"vosk-model-en-us-0.22")),(0,o.kt)("td",null,"1.8G"),(0,o.kt)("td",null,"5.69 (librispeech test-clean) 6.05 (tedlium) 29.78(callcenter)"),(0,o.kt)("td",null,"Accurate generic US English model"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-us-0.22-lgraph.zip"},"vosk-model-en-us-0.22-lgraph")),(0,o.kt)("td",null,"128M"),(0,o.kt)("td",null,"7.82 (librispeech) 8.20 (tedlium)"),(0,o.kt)("td",null,"Big US English model with dynamic graph"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"English Other")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,(0,o.kt)("strong",null,"Older Models")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-us-daanzu-20200905.zip"},"vosk-model-en-us-daanzu-20200905")),(0,o.kt)("td",null,"1.0G"),(0,o.kt)("td",null,"7.08 (librispeech test-clean)  8.25 (tedlium)"),(0,o.kt)("td",null,"Wideband model for dictation from ",(0,o.kt)("a",{href:"https://github.com/daanzu/kaldi-active-grammar"},"Kaldi-active-grammar")," project"),(0,o.kt)("td",null,"AGPL")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-us-daanzu-20200905-lgraph.zip"},"vosk-model-en-us-daanzu-20200905-lgraph")),(0,o.kt)("td",null,"129M"),(0,o.kt)("td",null,"8.20 (librispeech test-clean) 9.28 (tedlium)"),(0,o.kt)("td",null,"Wideband model for dictation from ",(0,o.kt)("a",{href:"https://github.com/daanzu/kaldi-active-grammar"},"Kaldi-active-grammar")," project with configurable graph"),(0,o.kt)("td",null,"AGPL")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-us-librispeech-0.2.zip"},"vosk-model-en-us-librispeech-0.2")),(0,o.kt)("td",null,"845M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Repackaged Librispeech model from ",(0,o.kt)("a",{href:"https://kaldi-asr.org/models/m13"},"Kaldi"),", not very accurate"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-en-us-zamia-0.5.zip"},"vosk-model-small-en-us-zamia-0.5")),(0,o.kt)("td",null,"49M"),(0,o.kt)("td",null,"11.55 (librispeech test-clean) 12.64 (tedlium)"),(0,o.kt)("td",null,"Repackaged Zamia model f_250, mainly for research"),(0,o.kt)("td",null,"LGPL-3.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-us-aspire-0.2.zip"},"vosk-model-en-us-aspire-0.2")),(0,o.kt)("td",null,"1.4G"),(0,o.kt)("td",null,"13.64 (librispeech test-clean) 12.89 (tedlium) 33.82(callcenter)"),(0,o.kt)("td",null,"Kaldi original ASPIRE model, not very accurate"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-us-0.21.zip"},"vosk-model-en-us-0.21")),(0,o.kt)("td",null,"1.6G"),(0,o.kt)("td",null,"5.43 (librispeech test-clean) 6.42 (tedlium) 40.63(callcenter)"),(0,o.kt)("td",null,"Wideband model previous generation"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Indian English")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-en-in-0.5.zip"},"vosk-model-en-in-0.5")),(0,o.kt)("td",null,"1G"),(0,o.kt)("td",null,"36.12 (NPTEL Pure)"),(0,o.kt)("td",null,"Generic Indian English model for telecom and broadcast"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-en-in-0.4.zip"},"vosk-model-small-en-in-0.4")),(0,o.kt)("td",null,"36M"),(0,o.kt)("td",null,"49.05 (NPTEL Pure)"),(0,o.kt)("td",null,"Lightweight Indian English model for mobile applications"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Chinese")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip"},"vosk-model-small-cn-0.22")),(0,o.kt)("td",null,"42M"),(0,o.kt)("td",null,"23.54 (SpeechIO-02) 38.29 (SpeechIO-06) 17.15 (THCHS)"),(0,o.kt)("td",null,"Lightweight model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-cn-0.22.zip"},"vosk-model-cn-0.22")),(0,o.kt)("td",null,"1.3G"),(0,o.kt)("td",null,"13.98 (SpeechIO-02) 27.30 (SpeechIO-06) 7.43 (THCHS)"),(0,o.kt)("td",null,"Big generic Chinese model for server processing"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Chinese Other")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-cn-kaldi-multicn-0.15.zip"},"vosk-model-cn-kaldi-multicn-0.15")),(0,o.kt)("td",null,"1.5G"),(0,o.kt)("td",null,"17.44 (SpeechIO-02) 9.56 (THCHS)"),(0,o.kt)("td",null,"Original Wideband Kaldi multi-cn model from ",(0,o.kt)("a",{href:"https://kaldi-asr.org/models/m11"},"Kaldi")," with Vosk LM"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Russian")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-ru-0.22.zip"},"vosk-model-ru-0.22")),(0,o.kt)("td",null,"1.5G"),(0,o.kt)("td",null,"5.74 (our audiobooks) 13.35 (open_stt audiobooks) 20.73 (open_stt youtube) 37.38 (openstt calls) 8.65 (golos crowd) 19.71 (sova devices)"),(0,o.kt)("td",null,"Big mixed band Russian model for server processing"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip"},"vosk-model-small-ru-0.22")),(0,o.kt)("td",null,"45M"),(0,o.kt)("td",null,"22.71 (openstt audiobooks) 31.97 (openstt youtube) 29.89 (sova devices) 11.79 (golos crowd)"),(0,o.kt)("td",null,"Lightweight wideband model for Android/iOS and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Russian Other")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-ru-0.10.zip"},"vosk-model-ru-0.10")),(0,o.kt)("td",null,"2.5G"),(0,o.kt)("td",null,"5.71 (our audiobooks) 16.26 (open_stt audiobooks) 26.20 (public_youtube_700_val open_stt) 40.15 (asr_calls_2_val open_stt)"),(0,o.kt)("td",null,"Big narrowband Russian model for server processing"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"French")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-fr-0.22.zip"},"vosk-model-small-fr-0.22")),(0,o.kt)("td",null,"41M"),(0,o.kt)("td",null,"23.95 (cv test) 19.30 (mtedx) 27.25 (podcast)"),(0,o.kt)("td",null,"Lightweight wideband model for Android/iOS and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-fr-0.22.zip"},"vosk-model-fr-0.22")),(0,o.kt)("td",null,"1.4G"),(0,o.kt)("td",null,"14.72 (cv test) 11.64 (mls) 13.10 (mtedx) 21.61 (podcast) 13.22 (voxpopuli)"),(0,o.kt)("td",null,"Big accurate model for servers"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"French Other")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-fr-pguyot-0.3.zip"},"vosk-model-small-fr-pguyot-0.3")),(0,o.kt)("td",null,"39M"),(0,o.kt)("td",null,"37.04 (cv test) 28.72 (mtedx) 37.46 (podcast)"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi trained by ",(0,o.kt)("a",{href:"https://github.com/pguyot/zamia-speech/releases"},"Paul Guyot")),(0,o.kt)("td",null,"CC-BY-NC-SA 4.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-fr-0.6-linto-2.2.0.zip"},"vosk-model-fr-0.6-linto-2.2.0")),(0,o.kt)("td",null,"1.5G"),(0,o.kt)("td",null,"16.19 (cv test) 16.44 (mtedx) 23.77 (podcast) 0.4xRT"),(0,o.kt)("td",null,"Model from ",(0,o.kt)("a",{href:"https://doc.linto.ai/#/services/linstt"},"LINTO")," project"),(0,o.kt)("td",null,"AGPL")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"German")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-de-0.21.zip"},"vosk-model-de-0.21")),(0,o.kt)("td",null,"1.9G"),(0,o.kt)("td",null,"9.83 (Tuda-de test), 24.00 (podcast) 12.82 (cv-test) 12.42 (mls) 33.26 (mtedx)"),(0,o.kt)("td",null,"Big German model for telephony and server"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-de-tuda-0.6-900k.zip"},"vosk-model-de-tuda-0.6-900k")),(0,o.kt)("td",null,"4.4G"),(0,o.kt)("td",null,"9.48 (Tuda-de test), 25.82 (podcast) 4.97 (cv-test) 11.01 (mls) 35.20 (mtedx)"),(0,o.kt)("td",null,"Latest big wideband model from ",(0,o.kt)("a",{href:"https://github.com/uhh-lt/kaldi-tuda-de"},"Tuda-DE")," project"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-de-zamia-0.3.zip"},"vosk-model-small-de-zamia-0.3")),(0,o.kt)("td",null,"49M"),(0,o.kt)("td",null,"14.81 (Tuda-de test, 37.46 (podcast)"),(0,o.kt)("td",null,"Zamia f_250 small model repackaged (not recommended)"),(0,o.kt)("td",null,"LGPL-3.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-de-0.15.zip"},"vosk-model-small-de-0.15")),(0,o.kt)("td",null,"45M"),(0,o.kt)("td",null,"13.75 (Tuda-de test), 30.67 (podcast)"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Spanish")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-es-0.42.zip"},"vosk-model-small-es-0.42")),(0,o.kt)("td",null,"39M"),(0,o.kt)("td",null,"16.02 (cv test) 16.72 (mtedx test) 11.21 (mls)"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-es-0.42.zip"},"vosk-model-es-0.42")),(0,o.kt)("td",null,"1.4G"),(0,o.kt)("td",null,"7.50 (cv test) 10.05 (mtedx test) 5.84 (mls)"),(0,o.kt)("td",null,"Big model for Spanish"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Portuguese/Brazilian Portuguese")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-pt-0.3.zip"},"vosk-model-small-pt-0.3")),(0,o.kt)("td",null,"31M"),(0,o.kt)("td",null,"68.92 (coraa dev) 32.60 (cv test)"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-pt-fb-v0.1.1-20220516_2113.zip"},"vosk-model-pt-fb-v0.1.1-20220516_2113")),(0,o.kt)("td",null,"1.6G"),(0,o.kt)("td",null,"54.34 (coraa dev) 27.70 (cv test)"),(0,o.kt)("td",null,"Big model from ",(0,o.kt)("a",{href:"https://gitlab.com/fb-resources/kaldi-br"},"FalaBrazil")),(0,o.kt)("td",null,"GPLv3.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Greek")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-el-gr-0.7.zip"},"vosk-model-el-gr-0.7")),(0,o.kt)("td",null,"1.1G"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Big narrowband Greek model for server processing, not extremely accurate though"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Turkish")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-tr-0.3.zip"},"vosk-model-small-tr-0.3")),(0,o.kt)("td",null,"35M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Vietnamese")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-vn-0.3.zip"},"vosk-model-small-vn-0.3")),(0,o.kt)("td",null,"32M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Italian")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-it-0.22.zip"},"vosk-model-small-it-0.22")),(0,o.kt)("td",null,"48M"),(0,o.kt)("td",null,"16.88 (cv test) 25.87 (mls) 17.01 (mtedx)"),(0,o.kt)("td",null,"Lightweight model for Android and RPi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-it-0.22.zip"},"vosk-model-it-0.22")),(0,o.kt)("td",null,"1.2G"),(0,o.kt)("td",null,"8.10 (cv test) 15.68 (mls) 11.23 (mtedx)"),(0,o.kt)("td",null,"Big generic Italian model for servers"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Dutch")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-nl-0.22.zip"},"vosk-model-small-nl-0.22")),(0,o.kt)("td",null,"39M"),(0,o.kt)("td",null,"22.45 (cv test) 26.80 (tv) 25.84 (mls) 24.09 (voxpopuli)"),(0,o.kt)("td",null,"Lightweight model for Dutch"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Dutch Other")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-nl-spraakherkenning-0.6.zip"},"vosk-model-nl-spraakherkenning-0.6")),(0,o.kt)("td",null,"860M"),(0,o.kt)("td",null,"20.40 (cv test) 32.64 (tv) 17.73 (mls) 19.96 (voxpopuli)"),(0,o.kt)("td",null,"Medium Dutch model from ",(0,o.kt)("a",{href:"https://github.com/opensource-spraakherkenning-nl/Kaldi_NL"},"Kaldi_NL")),(0,o.kt)("td",null,"CC-BY-NC-SA")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-nl-spraakherkenning-0.6-lgraph.zip"},"vosk-model-nl-spraakherkenning-0.6-lgraph")),(0,o.kt)("td",null,"100M"),(0,o.kt)("td",null,"22.82 (cv test) 34.01 (tv) 18.81 (mls) 21.01 (voxpopuli)"),(0,o.kt)("td",null,"Smaller model with dynamic graph"),(0,o.kt)("td",null,"CC-BY-NC-SA")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Catalan")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-ca-0.4.zip"},"vosk-model-small-ca-0.4")),(0,o.kt)("td",null,"42M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi for Catalan"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Arabic")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-ar-mgb2-0.4.zip"},"vosk-model-ar-mgb2-0.4")),(0,o.kt)("td",null,"318M"),(0,o.kt)("td",null,"16.40 (MGB-2 dev set)"),(0,o.kt)("td",null,"Repackaged Arabic model trained on MGB2 dataset from ",(0,o.kt)("a",{href:"https://kaldi-asr.org/models/m9"},"Kaldi")),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Farsi")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-fa-0.4.zip"},"vosk-model-small-fa-0.4")),(0,o.kt)("td",null,"47M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Lightweight wideband model for Android and RPi for Farsi (Persian)"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-fa-0.5.zip"},"vosk-model-fa-0.5")),(0,o.kt)("td",null,"1G"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Model with large vocabulary, not yet accurate but better than before (Persian)"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-fa-0.5.zip"},"vosk-model-small-fa-0.5")),(0,o.kt)("td",null,"60M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Bigger small model for desktop application (Persian)"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Filipino")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-tl-ph-generic-0.6.zip"},"vosk-model-tl-ph-generic-0.6")),(0,o.kt)("td",null,"320M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Medium wideband model for Filipino (Tagalog) by ",(0,o.kt)("a",{href:"https://github.com/feddybear/flipside_ph"},"feddybear")),(0,o.kt)("td",null,"CC-BY-NC-SA 4.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Ukrainian")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-uk-v3-nano.zip"},"vosk-model-small-uk-v3-nano")),(0,o.kt)("td",null,"73M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Nano model from ",(0,o.kt)("a",{href:"https://github.com/egorsmkv/speech-recognition-uk"},"Speech Recognition for Ukrainian")),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-uk-v3-small.zip"},"vosk-model-small-uk-v3-small")),(0,o.kt)("td",null,"133M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Small model from ",(0,o.kt)("a",{href:"https://github.com/egorsmkv/speech-recognition-uk"},"Speech Recognition for Ukrainian")),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-uk-v3.zip"},"vosk-model-uk-v3")),(0,o.kt)("td",null,"343M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Bigger model from ",(0,o.kt)("a",{href:"https://github.com/egorsmkv/speech-recognition-uk"},"Speech Recognition for Ukrainian")),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-uk-v3-lgraph.zip"},"vosk-model-uk-v3-lgraph")),(0,o.kt)("td",null,"325M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Big dynamic model from ",(0,o.kt)("a",{href:"https://github.com/egorsmkv/speech-recognition-uk"},"Speech Recognition for Ukrainian")),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Kazakh")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-kz-0.15.zip"},"vosk-model-small-kz-0.15")),(0,o.kt)("td",null,"42M"),(0,o.kt)("td",null,"9.60(dev) 8.32(test)"),(0,o.kt)("td",null,"Small mobile model from ",(0,o.kt)("a",{href:"https://github.com/IS2AI/ISSAI_SAIDA_Kazakh_ASR"},"SAIDA_Kazakh")),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-kz-0.15.zip"},"vosk-model-kz-0.15")),(0,o.kt)("td",null,"378M"),(0,o.kt)("td",null,"8.06(dev) 6.81(test)"),(0,o.kt)("td",null,"Bigger wideband model ",(0,o.kt)("a",{href:"https://github.com/IS2AI/ISSAI_SAIDA_Kazakh_ASR"},"SAIDA_Kazakh")),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Swedish")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-sv-rhasspy-0.15.zip"},"vosk-model-small-sv-rhasspy-0.15")),(0,o.kt)("td",null,"289M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Repackaged model from ",(0,o.kt)("a",{href:"https://github.com/rhasspy/sv_kaldi-rhasspy"},"Rhasspy project")),(0,o.kt)("td",null,"MIT")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Japanese")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-ja-0.22.zip"},"vosk-model-small-ja-0.22")),(0,o.kt)("td",null,"48M"),(0,o.kt)("td",null,"9.52(csj CER) 17.07(ted10k CER)"),(0,o.kt)("td",null,"Lightweight wideband model for Japanese"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-ja-0.22.zip"},"vosk-model-ja-0.22")),(0,o.kt)("td",null,"1Gb"),(0,o.kt)("td",null,"8.40(csj CER) 13.91(ted10k CER)"),(0,o.kt)("td",null,"Big model for Japanese"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Esperanto")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-eo-0.42.zip"},"vosk-model-small-eo-0.42")),(0,o.kt)("td",null,"42M"),(0,o.kt)("td",null,"7.24 (CV Test)"),(0,o.kt)("td",null,"Lightweight model for Esperanto"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Hindi")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-hi-0.22.zip"},"vosk-model-small-hi-0.22")),(0,o.kt)("td",null,"42M"),(0,o.kt)("td",null,"20.89 (IITM Challenge) 24.72 (MUCS Challenge)"),(0,o.kt)("td",null,"Lightweight model for Hindi"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-hi-0.22.zip"},"vosk-model-hi-0.22")),(0,o.kt)("td",null,"1.5Gb"),(0,o.kt)("td",null,"14.85 (CV Test) 14.83 (IITM Challenge) 13.11 (MUCS Challenge)"),(0,o.kt)("td",null,"Big accurate model for servers"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Czech")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-cs-0.4-rhasspy.zip"},"vosk-model-small-cs-0.4-rhasspy")),(0,o.kt)("td",null,"44M"),(0,o.kt)("td",null,"21.29 (CV Test)"),(0,o.kt)("td",null,"Lightweight model for Czech from Rhasspy project"),(0,o.kt)("td",null,"MIT")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Polish")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-small-pl-0.22.zip"},"vosk-model-small-pl-0.22")),(0,o.kt)("td",null,"50.5M"),(0,o.kt)("td",null,"18.36 (CV Test) 16.88 (MLS Test) 11.55 (Voxpopuli Test)"),(0,o.kt)("td",null,"Lightweight model for Polish for Android"),(0,o.kt)("td",null,"Apache 2.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("strong",null,"Speaker identification model")),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0"),(0,o.kt)("td",null,"\xa0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://alphacephei.com/vosk/models/vosk-model-spk-0.4.zip"},"vosk-model-spk-0.4")),(0,o.kt)("td",null,"13M"),(0,o.kt)("td",null,"TBD"),(0,o.kt)("td",null,"Model for speaker identification, should work for all languages"),(0,o.kt)("td",null,"Apache 2.0")))))}h.isMDXComponent=!0}}]);